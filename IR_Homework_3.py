# -*- coding: utf-8 -*-
"""IR Homework # 3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LXI7enIReem8BqUOeL1ljt7nuldi8oVL
"""

!pip install rank-bm25
!pip install nltk
!pip install scikit-learn
!pip install pandas
!pip install chardet
print("Packages installed successfully!")

"""Import Libraries"""

import chardet
import nltk
import os
import re
import json
import time
import numpy as np
import pandas as pd
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, RegexpTokenizer
from nltk.stem import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from rank_bm25 import BM25Okapi
from collections import defaultdict
import string
import matplotlib.pyplot as plt

print("Libraries imported successfully!")

"""Download NLTK Data"""

try:
    nltk.download('punkt')
    nltk.download('stopwords')
    nltk.download('punkt_tab')
    print(" NLTK data downloaded successfully!")
except Exception as e:
    print(f" NLTK download issue: {e}")
    print("Using fallback tokenization methods...")

regex_tokenizer = RegexpTokenizer(r'\w+')
print("Tokenizers initialized")

"""Data Loading Functions"""

def detect_encoding(file_path):
    """Detect file encoding"""
    with open(file_path, 'rb') as f:
        result = chardet.detect(f.read())
    return result['encoding']

def load_dataset():
    from google.colab import files
    print("Please upload dataset:")
    uploaded = files.upload()
    filename = list(uploaded.keys())[0]

    print(" Detecting file encoding..")
    encoding = detect_encoding(filename)
    print(f"Detected encoding: {encoding}")

    encodings_to_try = [encoding, 'latin-1', 'ISO-8859-1', 'cp1252', 'utf-8']

    df = None
    for enc in encodings_to_try:
        try:
            print(f"Trying encoding: {enc}")
            df = pd.read_csv(filename, encoding=enc)
            print(f"Successfully loaded with {enc} encoding")
            break
        except Exception as e:
            print(f"Failed with {enc}")
            continue

    if df is None:
        print("Using error-handling fallback...")
        df = pd.read_csv(filename, encoding='latin-1', errors='replace')

    return df, filename

df, filename = load_dataset()
print("Dataset loaded successfully!")

"""Dataset Exploration"""

print(" DATASET OVERVIEW")
print("=" * 50)
print(f"Dataset Shape: {df.shape}")
print(f"Columns: {df.columns.tolist()}")

print("\n NEWS TYPE DISTRIBUTION:")
print(df['NewsType'].value_counts())

print("\n SAMPLE DATA (First 3 rows):")
print(df.head(3))

print("\n MISSING VALUES ANALYSIS:")
print(df.isnull().sum())

print("\n DATA TYPES:")
print(df.dtypes)

print(" Dataset exploration completed!")

"""IR System Class"""

class AdvancedNewsIRSystem:
    def __init__(self):
        self.documents = []
        self.processed_docs = []
        self.doc_ids = []
        self.doc_metadata = []
        self.bm25 = None
        self.tfidf_vectorizer = None
        self.tfidf_matrix = None
        self.vocabulary = set()
        self.index = defaultdict(list)
        self.news_type_index = defaultdict(list)

    def load_from_dataframe(self, dataframe):
        print("Loading documents from dataframe.")
        self.documents = []
        self.doc_metadata = []

        for idx, row in dataframe.iterrows():
            try:
                article_text = str(row['Article']) if pd.notna(row['Article']) else ""
                heading_text = str(row['Heading']) if pd.notna(row['Heading']) else ""
                news_type = str(row['NewsType']) if pd.notna(row['NewsType']) else "unknown"
                date = str(row['Date']) if pd.notna(row['Date']) else "unknown"

                article_text = self.clean_text(article_text)
                heading_text = self.clean_text(heading_text)

                combined_text = f"{heading_text} {article_text}"
                self.documents.append(combined_text)
                self.doc_ids.append(idx)
                self.doc_metadata.append({
                    'heading': heading_text,
                    'date': date,
                    'news_type': news_type.lower(),
                    'original_article': article_text[:500] + "..." if len(article_text) > 500 else article_text
                })

                self.news_type_index[news_type.lower()].append(idx)
            except Exception as e:
                print(f"Warning: Error processing row {idx}: {e}")
                continue

        print(f"Successfully loaded {len(self.documents)} documents")

    def clean_text(self, text):
        if pd.isna(text):
            return ""

        text = str(text)
        text = re.sub(r'[^\x00-\x7F]+', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()

    def preprocess_text(self, text):
        if pd.isna(text) or text == "":
            return []

        text = str(text)

        text = text.lower()

        text = re.sub(r'[^a-zA-Z0-9\s]', ' ', text)

        try:
            tokens = word_tokenize(text)
        except:
            tokens = regex_tokenizer.tokenize(text)

        try:
            stop_words = set(stopwords.words('english'))
        except:
            stop_words = set(['the', 'a', 'an', 'in', 'on', 'at', 'for', 'to', 'of', 'and', 'or', 'but'])

        extended_stopwords = ['said', 'also', 'would', 'could', 'according', 'year', 'years', 'time', 'new']
        stop_words.update(extended_stopwords)

        tokens = [token for token in tokens if token not in stop_words and len(token) > 2]

        stemmer = PorterStemmer()
        tokens = [stemmer.stem(token) for token in tokens]

        return tokens

    def build_index(self):
        print("Preprocessing documents...")
        self.processed_docs = [self.preprocess_text(doc) for doc in self.documents]

        print("Building inverted index...")
        for doc_id, tokens in enumerate(self.processed_docs):
            for position, token in enumerate(tokens):
                self.index[token].append((doc_id, position))
            self.vocabulary.update(tokens)

        print("Building BM25 model...")
        self.bm25 = BM25Okapi(self.processed_docs)

        print("Building TF-IDF model...")
        doc_strings = [' '.join(tokens) for tokens in self.processed_docs]
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=15000,
            min_df=2,
            max_df=0.8,
            ngram_range=(1, 2)
        )
        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(doc_strings)

        print(f"Index built with {len(self.vocabulary)} unique terms")

    def boolean_search(self, query, news_type=None):
        query_terms = self.preprocess_text(query)
        if not query_terms:
            return []

        phrase_terms = re.findall(r'"([^"]*)"', query)
        if phrase_terms:
            phrase_docs = self._phrase_search(phrase_terms[0])
            query = re.sub(r'"[^"]*"', '', query)
            query_terms = self.preprocess_text(query)

            if query_terms:
                term_docs = self._boolean_and_search(query_terms)
                result_docs = phrase_docs.intersection(term_docs)
            else:
                result_docs = phrase_docs
        else:
            result_docs = self._boolean_and_search(query_terms)

        if news_type:
            type_docs = set(self.news_type_index.get(news_type.lower(), []))
            result_docs = result_docs.intersection(type_docs)

        return list(result_docs)

    def _boolean_and_search(self, query_terms):
        doc_sets = []
        for term in query_terms:
            if term in self.index:
                doc_sets.append(set(doc_id for doc_id, _ in self.index[term]))
            else:
                doc_sets.append(set())

        return set.intersection(*doc_sets) if doc_sets else set()

    def _phrase_search(self, phrase):
        phrase_terms = self.preprocess_text(phrase)
        if len(phrase_terms) < 2:
            return set()

        doc_sets = []
        for term in phrase_terms:
            if term in self.index:
                doc_sets.append(set(doc_id for doc_id, _ in self.index[term]))
            else:
                return set()

        common_docs = set.intersection(*doc_sets)

        phrase_docs = set()
        for doc_id in common_docs:
            positions = {}
            for term in phrase_terms:
                positions[term] = [pos for doc, pos in self.index[term] if doc == doc_id]

            first_term_positions = positions[phrase_terms[0]]
            for pos in first_term_positions:
                phrase_found = True
                for i, term in enumerate(phrase_terms[1:], 1):
                    if pos + i not in positions[term]:
                        phrase_found = False
                        break
                if phrase_found:
                    phrase_docs.add(doc_id)
                    break

        return phrase_docs

    def bm25_search(self, query, top_k=10, news_type=None):
        query_terms = self.preprocess_text(query)
        if not query_terms or not self.bm25:
            return []

        scores = self.bm25.get_scores(query_terms)
        if news_type:
            type_docs = set(self.news_type_index.get(news_type.lower(), []))
            filtered_results = []
            for idx, score in enumerate(scores):
                if idx in type_docs and score > 0:
                    filtered_results.append((idx, score))
            filtered_results.sort(key=lambda x: x[1], reverse=True)
            return filtered_results[:top_k]
        else:
            top_indices = np.argsort(scores)[::-1][:top_k]
            return [(idx, scores[idx]) for idx in top_indices if scores[idx] > 0]

    def tfidf_search(self, query, top_k=10, news_type=None):
        query_vec = self.tfidf_vectorizer.transform([' '.join(self.preprocess_text(query))])
        scores = (self.tfidf_matrix * query_vec.T).toarray().flatten()

        if news_type:
            type_docs = set(self.news_type_index.get(news_type.lower(), []))
            filtered_results = []
            for idx, score in enumerate(scores):
                if idx in type_docs and score > 0:
                    filtered_results.append((idx, score))
            filtered_results.sort(key=lambda x: x[1], reverse=True)
            return filtered_results[:top_k]
        else:
            top_indices = np.argsort(scores)[::-1][:top_k]
            return [(idx, scores[idx]) for idx in top_indices if scores[idx] > 0]

    def hybrid_search(self, query, top_k=10, news_type=None):

        bm25_results = self.bm25_search(query, top_k=top_k*3, news_type=news_type)

        if not bm25_results:
            return []

        candidate_docs = [idx for idx, _ in bm25_results]
        query_vec = self.tfidf_vectorizer.transform([' '.join(self.preprocess_text(query))])

        candidate_scores = []
        for doc_id in candidate_docs:
            score = (self.tfidf_matrix[doc_id] * query_vec.T).toarray()[0][0]
            candidate_scores.append((doc_id, score))

        candidate_scores.sort(key=lambda x: x[1], reverse=True)
        return candidate_scores[:top_k]

    def evaluate_query(self, query, top_k=10, method='hybrid', news_type=None):
        start_time = time.time()

        if method == 'boolean':
            results = self.boolean_search(query, news_type)
            scored_results = [(doc_id, 1.0) for doc_id in results][:top_k]
        elif method == 'bm25':
            scored_results = self.bm25_search(query, top_k, news_type)
        elif method == 'tfidf':
            scored_results = self.tfidf_search(query, top_k, news_type)
        else:
            scored_results = self.hybrid_search(query, top_k, news_type)

        processing_time = time.time() - start_time

        return {
            'query': query,
            'results': scored_results,
            'num_results': len(scored_results),
            'processing_time': processing_time,
            'method': method,
            'news_type_filter': news_type
        }

    def get_document_details(self, doc_id):
        return {
            'doc_id': doc_id,
            'heading': self.doc_metadata[doc_id]['heading'],
            'date': self.doc_metadata[doc_id]['date'],
            'news_type': self.doc_metadata[doc_id]['news_type'],
            'article_preview': self.doc_metadata[doc_id]['original_article'],
            'full_article': self.documents[doc_id]
        }

    def get_system_stats(self):
        news_types = [meta['news_type'] for meta in self.doc_metadata]
        doc_lengths = [len(doc) for doc in self.processed_docs]

        return {
            'total_documents': len(self.documents),
            'vocabulary_size': len(self.vocabulary),
            'total_terms': sum(len(doc) for doc in self.processed_docs),
            'average_document_length': np.mean(doc_lengths),
            'std_document_length': np.std(doc_lengths),
            'news_type_distribution': {nt: news_types.count(nt) for nt in set(news_types)},
            'inverted_index_size': sum(len(positions) for positions in self.index.values())
        }

print(" Complete IR System class defined successfully!")

"""Evaluation and Analysis Class"""

class NewsIREvaluator:
    def __init__(self, ir_system):
        self.ir_system = ir_system
        self.results_history = []

    def comprehensive_evaluation(self):
        print("=== IR SYSTEM EVALUATION ===\n")

        test_queries = [
            # Business queries
            ("stock market", None, "General business news"),
            ("company earnings", "business", "Business-specific"),
            ("economic growth", "business", "Business economic"),
            ("investment", "business", "Business investment"),

            # Sports queries
            ("football", None, "General sports news"),
            ("basketball", "sports", "Sports-specific"),
            ("player transfer", "sports", "Sports transfers"),
            ("team", "sports", "Sports teams"),

            # Mixed queries
            ("market", None, "Ambiguous - both domains"),
            ("game", None, "Ambiguous - both domains")
        ]

        methods = ['boolean', 'bm25', 'tfidf', 'hybrid']
        evaluation_results = {}

        for method in methods:
            print(f"\n{'='*50}")
            print(f"EVALUATING {method.upper()} METHOD")
            print(f"{'='*50}")

            method_results = []
            for query, news_type, description in test_queries:
                result = self.ir_system.evaluate_query(
                    query,
                    top_k=5,
                    method=method,
                    news_type=news_type
                )

                method_results.append(result)

                print(f"\n {description}")
                print(f"   Query: '{query}' | Filter: {news_type}")
                print(f"   Found: {result['num_results']} results | Time: {result['processing_time']:.4f}s")

                # Show top results
                for i, (doc_id, score) in enumerate(result['results'][:2]):
                    doc_info = self.ir_system.get_document_details(doc_id)
                    print(f"   {i+1}. [{doc_info['news_type'].upper()}] {doc_info['heading'][:60]}... (Score: {score:.4f})")

            evaluation_results[method] = method_results
            self.results_history.extend(method_results)

        return evaluation_results

    def performance_analysis(self):
        print("\n" + "="*60)
        print("PERFORMANCE ANALYSIS SUMMARY")
        print("="*60)

        method_stats = {}
        for method in ['boolean', 'bm25', 'tfidf', 'hybrid']:
            method_results = [r for r in self.results_history if r['method'] == method]
            if method_results:
                avg_time = np.mean([r['processing_time'] for r in method_results])
                avg_results = np.mean([r['num_results'] for r in method_results])
                method_stats[method] = {
                    'avg_processing_time': avg_time,
                    'avg_results_per_query': avg_results,
                    'total_queries': len(method_results)
                }

                print(f"\n{method.upper():<10} | Avg Time: {avg_time:.4f}s | Avg Results: {avg_results:.1f} | Queries: {len(method_results)}")

        return method_stats

    def create_performance_chart(self, method_stats):
        methods = list(method_stats.keys())
        times = [method_stats[m]['avg_processing_time'] for m in methods]
        results = [method_stats[m]['avg_results_per_query'] for m in methods]

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))


        ax1.bar(methods, times, color=['#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4'])
        ax1.set_title('Average Query Processing Time')
        ax1.set_ylabel('Time (seconds)')
        ax1.set_xlabel('Retrieval Method')


        ax2.bar(methods, results, color=['#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4'])
        ax2.set_title('Average Results per Query')
        ax2.set_ylabel('Number of Results')
        ax2.set_xlabel('Retrieval Method')

        plt.tight_layout()
        plt.show()

"""Initialize and Build the IR System"""

print(" INITIALIZING NEWS INFORMATION RETRIEVAL SYSTEM")
print("=" * 30)


news_ir = AdvancedNewsIRSystem()


news_ir.load_from_dataframe(df)

print("\n BUILDING SEARCH INDICES...")
news_ir.build_index()

"""Display System Statistics"""

#Display System Statistics
print(" SYSTEM STATISTICS")
print("=" * 40)

stats = news_ir.get_system_stats()
for key, value in stats.items():
    if key == 'news_type_distribution':
        print(f"\n{key}:")
        for news_type, count in value.items():
            print(f"   {news_type}: {count} documents")
    else:
        print(f" {key}: {value}")

print("\n System statistics displayed!")

"""Run Evaluation"""

print("RUNNING  EVALUATION")
print("=" * 50)

evaluator = NewsIREvaluator(news_ir)

evaluation_results = evaluator.comprehensive_evaluation()

print("Evaluation completed!")

"""Performance Analysis and Visualization"""

print("PERFORMANCE ANALYSIS")
print("=" * 40)

method_stats = evaluator.performance_analysis()

print("\nGenerating Perormance Charts...")
evaluator.create_performance_chart(method_stats)

"""Advanced Features"""

# Advanced Features
print(" ADVANCED FEATURES")
print("=" * 50)


print("\n PHRASE SEARCH:")
phrase_result = news_ir.evaluate_query('"stock market"', method='boolean')
print(f"Query: 'stock market' | Results: {phrase_result['num_results']}")
for i, (doc_id, score) in enumerate(phrase_result['results'][:3]):
    doc_info = news_ir.get_document_details(doc_id)
    print(f"  {i+1}. {doc_info['heading'][:70]}...")


print("\n NEWS TYPE FILTERING:")
filtered_result = news_ir.evaluate_query("game", news_type="sports", method='hybrid')
print(f"Query: 'game' + Sports filter | Results: {filtered_result['num_results']}")
for i, (doc_id, score) in enumerate(filtered_result['results'][:3]):
    doc_info = news_ir.get_document_details(doc_id)
    print(f"  {i+1}. [{doc_info['news_type'].upper()}] {doc_info['heading'][:70]}...")

"""Interactive Query Testing"""

# Interactive Query Testing
def test_custom_query():
    print(" INTERACTIVE QUERY TESTING")
    print("=" * 40)

    query = input("Enter query: ")
    news_type = input("Filter by news type (business/sports/leave blank for all): ").lower()
    if news_type == "":
        news_type = None

    methods = ['boolean', 'bm25', 'tfidf', 'hybrid']

    for method in methods:
        print(f"\nTesting {method.upper()} method:")
        result = news_ir.evaluate_query(query, method=method, news_type=news_type)
        print(f"   Found: {result['num_results']} results | Time: {result['processing_time']:.4f}s")

        for i, (doc_id, score) in enumerate(result['results'][:3]):
            doc_info = news_ir.get_document_details(doc_id)
            print(f"   {i+1}. [{doc_info['news_type'].upper()}] {doc_info['heading']} (Score: {score:.4f})")


test_custom_query()

#Simple Single Query Test
def simple_query_test():
    """Run a simple test with one query"""
    print(" SIMPLE QUERY TEST")
    print("=" * 40)

    query = "wrestling"
    print(f"Testing query: '{query}'")

    methods = ['boolean', 'bm25', 'tfidf', 'hybrid']

    for method in methods:
        print(f"\n {method.upper()} Method:")
        result = news_ir.evaluate_query(query, method=method, top_k=3)
        print(f"   Found: {result['num_results']} results | ⏱️ Time: {result['processing_time']:.4f}s")

        for i, (doc_id, score) in enumerate(result['results']):
            doc_info = news_ir.get_document_details(doc_id)
            print(f"   {i+1}. [{doc_info['news_type'].upper()}] {doc_info['heading']} (Score: {score:.4f})")


simple_query_test()

"""System Summary"""

# System Summary and Completion
print(" INFORMATION RETRIEVAL SYSTEM - COMPLETE SUMMARY")
print("=" * 60)
print(" System successfully built and evaluated!")
print(" All retrieval methods implemented:")
print("   - Boolean Retrieval")
print("   - BM25 Probabilistic Retrieval")
print("   - TF-IDF Vector Space Retrieval")
print("   - Hybrid BM25+TF-IDF Retrieval")
print("Advanced features available:")
print("   - Phrase search with quotes")
print("   - News type filtering (business/sports)")
print("   - Performance comparison charts")
print(" Ready for technical report generation!")

print(f"\n Final System Stats:")
stats = news_ir.get_system_stats()
print(f"   Documents: {stats['total_documents']}")
print(f"   Vocabulary: {stats['vocabulary_size']} terms")
print(f"   News Types: {stats['news_type_distribution']}")

